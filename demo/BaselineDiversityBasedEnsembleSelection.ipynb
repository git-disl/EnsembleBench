{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c22b5df4",
   "metadata": {},
   "source": [
    "# Baseline Diversity-based Ensemble Selection\n",
    "\n",
    "This demo provides the baseline diversity-based ensemble selection examples on CIFAR-10 and ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d4f7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import timeit\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from itertools import combinations\n",
    "\n",
    "from EnsembleBench.frameworks.pytorchUtility import (\n",
    "    calAccuracy,\n",
    "    calAveragePredictionVectorAccuracy,\n",
    "    calNegativeSamplesSet,\n",
    "    calDisagreementSamplesNoGroundTruth,\n",
    "    filterModelsFixed,\n",
    ")\n",
    "\n",
    "from EnsembleBench.groupMetrics import (\n",
    "    calAllDiversityMetrics,\n",
    ")\n",
    "from EnsembleBench.teamSelection import (\n",
    "    getNTeamStatistics,\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bf108a",
   "metadata": {},
   "source": [
    "## Dataset Configurations\n",
    "\n",
    "You can download the extracted predictions for CIFAR-10 and ImageNet from the following Google Drive folder.\n",
    "https://drive.google.com/drive/folders/18rEcjSpMSy-XN2bUQ3PfsBppwb874B8q?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfabb251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply use the extracted prediction results to calculate the diversity scores and perform ensemble selection\n",
    "\n",
    "dataset = 'cifar10'\n",
    "diversityMetricsList = ['CK', 'QS', 'BD', 'FK', 'KW', 'GD']\n",
    "\n",
    "if dataset == 'cifar10':\n",
    "    predictionDir = './cifar10/prediction'\n",
    "    models = ['densenet-L190-k40', 'densenetbc-100-12', 'resnext8x64d', 'wrn-28-10-drop', 'vgg19_bn', \n",
    "              'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110']\n",
    "    maxModel = 0\n",
    "    maxModelAcc = 96.68\n",
    "    targetAcc = 96.33 # accuracy of entire ensemble\n",
    "elif dataset == 'imagenet':\n",
    "    predictionDir = './imagenet/prediction'\n",
    "    models = np.array(['AlexNet', 'DenseNet', 'EfficientNetb0', 'ResNeXt50', 'Inception3', 'ResNet152', 'ResNet18', 'SqueezeNet', 'VGG16', 'VGG19bn'])\n",
    "    maxModel = 5\n",
    "    maxModelAcc = 78.25\n",
    "    targetAcc = 79.82 # accuracy of entire ensemble\n",
    "\n",
    "else:\n",
    "    raise Exception(\"Dataset not support!\")\n",
    "\n",
    "suffix = '.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc4ef1e",
   "metadata": {},
   "source": [
    "# Perform Baseline Diversity-based Ensemble Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b1f777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prediction vectors\n",
    "labelVectorsList = list()\n",
    "predictionVectorsList = list()\n",
    "tmpAccList = list()\n",
    "for m in models:\n",
    "    predictionPath = os.path.join(predictionDir, m+suffix)\n",
    "    prediction = torch.load(predictionPath)\n",
    "    predictionVectors = prediction['predictionVectors']\n",
    "    predictionVectorsList.append(torch.nn.functional.softmax(predictionVectors, dim=-1).cpu())\n",
    "    labelVectors = prediction['labelVectors']\n",
    "    labelVectorsList.append(labelVectors.cpu())\n",
    "    tmpAccList.append(calAccuracy(predictionVectors, labelVectors)[0].cpu())\n",
    "    print(tmpAccList[-1])\n",
    "\n",
    "minAcc = np.min(tmpAccList)\n",
    "avgAcc = np.mean(tmpAccList)\n",
    "maxAcc = np.max(tmpAccList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630b1d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain negative samples for any base models\n",
    "sampleID, sampleTarget, predictions, predVectors = calDisagreementSamplesNoGroundTruth(\n",
    "    predictionVectorsList, labelVectorsList[0]\n",
    ")\n",
    "\n",
    "sampleID = np.array(sampleID)\n",
    "sampleTarget = np.array(sampleTarget)\n",
    "predictions = np.array(predictions)\n",
    "predVectors = np.array([np.array([np.array(pp) for pp in p]) for p in predVectors])\n",
    "\n",
    "# settings for the diversity score calculation\n",
    "nModels = len(predictions[0])\n",
    "modelIdx = list(range(nModels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07373f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate diversity scores for ensemble teams\n",
    "np.random.seed(0)\n",
    "crossValidation = True\n",
    "crossValidationTimes = 3\n",
    "nRandomSamples = 100\n",
    "\n",
    "teamSizeList = list()\n",
    "teamList = list()\n",
    "diversityScoresList = list()\n",
    "\n",
    "startTime = timeit.default_timer()\n",
    "for n in range(2, nModels+1):\n",
    "    comb = combinations(modelIdx, n)\n",
    "    for selectedModels in list(comb):\n",
    "        teamSampleID, teamSampleTarget, teamPredictions, teamPredVectors = filterModelsFixed(sampleID, sampleTarget, predictions, predVectors, selectedModels) \n",
    "        \n",
    "        if len(teamPredictions) == 0:\n",
    "            print(\"negative sample not found\")\n",
    "            continue\n",
    "        \n",
    "        if crossValidation:\n",
    "            tmpMetrics = list()   \n",
    "            for _ in range(crossValidationTimes):\n",
    "                randomIdx = np.random.choice(np.arange(teamPredictions.shape[0]), nRandomSamples)\n",
    "                tmpMetrics.append(calAllDiversityMetrics(teamPredictions[randomIdx], teamSampleTarget[randomIdx], diversityMetricsList))\n",
    "            tmpMetrics = np.mean(np.array(tmpMetrics), axis=0)\n",
    "        else:\n",
    "            tmpMetrics = np.array(calAllDiversityMetrics(teamPredictions, teamSampleTarget, diversityMetricsList))\n",
    "        \n",
    "        diversityScoresList.append(tmpMetrics)                                  \n",
    "        teamSizeList.append(n)\n",
    "        teamList.append(selectedModels)\n",
    "endTime = timeit.default_timer()\n",
    "print(\"Time: \", endTime-startTime)\n",
    "\n",
    "diversityScoresList = np.array(diversityScoresList)\n",
    "teamSizeList = np.array(teamSizeList)\n",
    "teamList = np.array(teamList, dtype=object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0480be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform mean-threshold based ensemble selection (baseline approach)\n",
    "QMetrics = {}\n",
    "QMetricsThreshold = {}\n",
    "teamSelectedQAllDict = {}\n",
    "\n",
    "for j, dm in enumerate(diversityMetricsList):\n",
    "    QMetricsThreshold[dm] = np.mean(diversityScoresList[..., j])\n",
    "\n",
    "print(\"Diversity threshold: \", QMetricsThreshold)\n",
    "\n",
    "for i, t in enumerate(teamList):\n",
    "    teamName = \"\".join(map(str, t))\n",
    "    for j, dm in enumerate(diversityMetricsList):\n",
    "        QMetricsDM = QMetrics.get(dm, {})\n",
    "        QMetricsDM[teamName] = diversityScoresList[i][j]\n",
    "        QMetrics[dm] = QMetricsDM\n",
    "        if QMetricsDM[teamName] > round(QMetricsThreshold[dm], 3):\n",
    "            teamSelectedQAllSet = teamSelectedQAllDict.get(dm, set())\n",
    "            teamSelectedQAllSet.add(teamName)\n",
    "            teamSelectedQAllDict[dm] = teamSelectedQAllSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c554be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ensemble selection results\n",
    "\n",
    "# Calculate the team accuracy (optional)\n",
    "# team -> accuracy map\n",
    "# model -> team\n",
    "import timeit\n",
    "teamAccuracyDict = dict()\n",
    "startTime = timeit.default_timer()\n",
    "for n in range(2, len(models)+1):\n",
    "    comb = combinations(list(range(len(models))), n)\n",
    "    for selectedModels in list(comb):\n",
    "        tmpAccuracy = calAveragePredictionVectorAccuracy(predictionVectorsList, labelVectorsList[0], modelsList=selectedModels)[0].cpu().item()\n",
    "        teamName = \"\".join(map(str, selectedModels))\n",
    "        teamAccuracyDict[teamName] = tmpAccuracy\n",
    "endTime = timeit.default_timer()\n",
    "print(\"Accuracy Calculation Time (s): \", endTime-startTime)\n",
    "\n",
    "# statistics for different diversity metrics\n",
    "for dm in diversityMetricsList:\n",
    "    print(dm, getNTeamStatistics(list(teamSelectedQAllDict[dm]), teamAccuracyDict,\n",
    "                                 minAcc, avgAcc, maxAcc, tmpAccList))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
