{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "031d9572",
   "metadata": {},
   "source": [
    "# Focal Diversity-based Ensemble Selection\n",
    "\n",
    "This demo provides the focal diversity-based ensemble selection examples on CIFAR-10 and ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f433d980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import timeit\n",
    "\n",
    "# EnsembleBench modules\n",
    "from EnsembleBench.frameworks.pytorchUtility import (\n",
    "    calAccuracy,\n",
    "    calAveragePredictionVectorAccuracy,\n",
    "    calNegativeSamplesSet,\n",
    "    calDisagreementSamplesOneTargetNegative,\n",
    "    filterModelsFixed,\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9e3966",
   "metadata": {},
   "source": [
    "## Dataset Configurations\n",
    "\n",
    "You can download the extracted predictions for CIFAR-10 and ImageNet from the following Google Drive folder.\n",
    "https://drive.google.com/drive/folders/18rEcjSpMSy-XN2bUQ3PfsBppwb874B8q?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75188ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply use the extracted prediction results to calculate the diversity scores and perform ensemble selection\n",
    "\n",
    "dataset = 'cifar10'\n",
    "diversityMetricsList = ['CK', 'QS', 'BD', 'FK', 'KW', 'GD']\n",
    "\n",
    "if dataset == 'cifar10':\n",
    "    predictionDir = './cifar10/prediction'\n",
    "    models = ['densenet-L190-k40', 'densenetbc-100-12', 'resnext8x64d', 'wrn-28-10-drop', 'vgg19_bn', \n",
    "              'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110']\n",
    "    maxModel = 0\n",
    "    maxModelAcc = 96.68\n",
    "    targetAcc = 96.33 # accuracy of entire ensemble\n",
    "elif dataset == 'imagenet':\n",
    "    predictionDir = './imagenet/prediction'\n",
    "    models = np.array(['AlexNet', 'DenseNet', 'EfficientNetb0', 'ResNeXt50', 'Inception3', 'ResNet152', 'ResNet18', 'SqueezeNet', 'VGG16', 'VGG19bn'])\n",
    "    maxModel = 5\n",
    "    maxModelAcc = 78.25\n",
    "    targetAcc = 79.82 # accuracy of entire ensemble\n",
    "\n",
    "else:\n",
    "    raise Exception(\"Dataset not support!\")\n",
    "\n",
    "suffix = '.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d3ebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelVectorsList = list()\n",
    "predictionVectorsList = list()\n",
    "tmpAccList = list()\n",
    "for m in models:\n",
    "    predictionPath = os.path.join(predictionDir, m+suffix)\n",
    "    prediction = torch.load(predictionPath)\n",
    "    predictionVectors = prediction['predictionVectors']\n",
    "    predictionVectorsList.append(torch.nn.functional.softmax(predictionVectors, dim=-1).cpu())\n",
    "    labelVectors = prediction['labelVectors']\n",
    "    labelVectorsList.append(labelVectors.cpu())\n",
    "    tmpAccList.append(calAccuracy(predictionVectors, labelVectors)[0].cpu())\n",
    "    print(tmpAccList[-1])\n",
    "\n",
    "minAcc = np.min(tmpAccList)\n",
    "avgAcc = np.mean(tmpAccList)\n",
    "maxAcc = np.max(tmpAccList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f776923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "# team -> accuracy map\n",
    "# model -> team\n",
    "teamAccuracyDict = dict()\n",
    "modelTeamDict = dict()\n",
    "teamNameDict = dict()\n",
    "startTime = timeit.default_timer()\n",
    "for n in range(2, len(models)+1):\n",
    "    comb = combinations(list(range(len(models))), n)\n",
    "    for selectedModels in list(comb):\n",
    "        tmpAccuracy = calAveragePredictionVectorAccuracy(predictionVectorsList, labelVectorsList[0], modelsList=selectedModels)[0].cpu().item()\n",
    "        teamName = \"\".join(map(str, selectedModels))\n",
    "        teamNameDict[teamName] = selectedModels\n",
    "        teamAccuracyDict[teamName] = tmpAccuracy\n",
    "        for m in teamName:\n",
    "            if m in modelTeamDict:\n",
    "                modelTeamDict[m].add(teamName)\n",
    "            else:\n",
    "                modelTeamDict[m] = set([teamName,])\n",
    "endTime = timeit.default_timer()\n",
    "print(\"Time: \", endTime-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d577abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the diversity measures for all configurations\n",
    "import numpy as np\n",
    "from EnsembleBench.groupMetrics import *\n",
    "np.random.seed(0)\n",
    "nRandomSamples = 100\n",
    "crossValidation = True\n",
    "crossValidationTimes = 3\n",
    "\n",
    "teamDiversityMetricMap = dict()\n",
    "negAccuracyDict = dict()\n",
    "startTime = timeit.default_timer()\n",
    "for oneTargetModel in range(len(models)):\n",
    "    sampleID, sampleTarget, predictions, predVectors = calDisagreementSamplesOneTargetNegative(predictionVectorsList, labelVectorsList[0], oneTargetModel)\n",
    "    if len(predictions) == 0:\n",
    "        print(\"negative sample not found\")\n",
    "        continue\n",
    "    sampleID = np.array(sampleID)\n",
    "    sampleTarget = np.array(sampleTarget)\n",
    "    predictions = np.array(predictions)\n",
    "    predVectors = np.array([np.array([np.array(pp) for pp in p]) for p in predVectors])\n",
    "    for teamName in modelTeamDict[str(oneTargetModel)]:\n",
    "        selectedModels = teamNameDict[teamName]\n",
    "        teamSampleID, teamSampleTarget, teamPredictions, teamPredVectors = filterModelsFixed(sampleID, sampleTarget, predictions, predVectors, selectedModels) \n",
    "        if crossValidation:\n",
    "            tmpMetrics = list()\n",
    "            for _ in range(crossValidationTimes):\n",
    "                randomIdx = np.random.choice(np.arange(teamPredictions.shape[0]), nRandomSamples)        \n",
    "                tmpMetrics.append(calAllDiversityMetrics(teamPredictions[randomIdx], teamSampleTarget[randomIdx], diversityMetricsList))\n",
    "            tmpMetrics = np.mean(np.array(tmpMetrics), axis=0)\n",
    "        else:\n",
    "            tmpMetrics = np.array(calAllDiversityMetrics(teamPredictions, teamSampleTarget, diversityMetricsList))\n",
    "        diversityMetricDict = {diversityMetricsList[i]:tmpMetrics[i].item()  for i in range(len(tmpMetrics))}\n",
    "        targetDiversity = teamDiversityMetricMap.get(teamName, dict())\n",
    "        targetDiversity[str(oneTargetModel)] = diversityMetricDict\n",
    "        teamDiversityMetricMap[teamName] = targetDiversity\n",
    "        \n",
    "        tmpNegAccuracy = calAccuracy(torch.tensor(np.mean(np.transpose(teamPredVectors, (1, 0, 2)), axis=0)), torch.tensor(teamSampleTarget))[0].cpu().item()\n",
    "        targetNegAccuracy = negAccuracyDict.get(teamName, dict())\n",
    "        targetNegAccuracy[str(oneTargetModel)] = tmpNegAccuracy\n",
    "        negAccuracyDict[teamName] = targetNegAccuracy\n",
    "\n",
    "endTime = timeit.default_timer()\n",
    "print(\"Time: \", endTime-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc991e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the targetTeamSizeDict\n",
    "startTime = timeit.default_timer()\n",
    "targetTeamSizeDict = dict()\n",
    "for oneTargetModel in range(len(models)):\n",
    "    for teamName in modelTeamDict[str(oneTargetModel)]:\n",
    "        teamSize = len(teamName)\n",
    "        teamSizeDict = targetTeamSizeDict.get(str(oneTargetModel), dict())\n",
    "        fixedTeamDict = teamSizeDict.get(str(teamSize), dict())\n",
    "        \n",
    "        teamList = fixedTeamDict.get('TeamList', list())\n",
    "        teamList.append(teamName)\n",
    "        fixedTeamDict['TeamList'] = teamList\n",
    "        \n",
    "        # diversity measures\n",
    "        diversityVector = np.expand_dims(np.array([teamDiversityMetricMap[teamName][str(oneTargetModel)][dm]\n",
    "                                    for dm in diversityMetricsList]), axis=0)\n",
    "        \n",
    "        diversityMatrix = fixedTeamDict.get('DiversityMatrix', None)\n",
    "        if diversityMatrix is None:\n",
    "            diversityMatrix = diversityVector\n",
    "        else:\n",
    "            diversityMatrix = np.append(diversityMatrix, diversityVector, axis=0)\n",
    "        fixedTeamDict['DiversityMatrix'] = diversityMatrix\n",
    "        \n",
    "        teamSizeDict[str(teamSize)] = fixedTeamDict\n",
    "        targetTeamSizeDict[str(oneTargetModel)] = teamSizeDict \n",
    "endTime = timeit.default_timer()\n",
    "print(\"Time: \", endTime-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e907336",
   "metadata": {},
   "outputs": [],
   "source": [
    "teamSelectedFQDict = dict()\n",
    "teamSelectedFQOutDict = dict()\n",
    "from EnsembleBench.teamSelection import *\n",
    "for oneTargetModel in range(len(models)):\n",
    "    targetFQDict = teamSelectedFQDict.get(str(oneTargetModel), dict())\n",
    "    targetFQOutDict = teamSelectedFQOutDict.get(str(oneTargetModel), dict())\n",
    "    for teamSize in range(2, len(models)):\n",
    "        targetTeamSizeFQDict = targetFQDict.get(str(teamSize), dict())\n",
    "        targetTeamSizeFQOutDict = targetFQOutDict.get(str(teamSize), dict())\n",
    "        fixedTeamDict = targetTeamSizeDict[str(oneTargetModel)][str(teamSize)]\n",
    "        thresholds = list()\n",
    "        kmeans = list()\n",
    "        teamList = fixedTeamDict['TeamList']\n",
    "        accuracyList = [teamAccuracyDict[teamName] for teamName in teamList]\n",
    "        diversityMatrix = fixedTeamDict['DiversityMatrix']\n",
    "        for i in range(len(diversityMetricsList)):\n",
    "            tmpThreshold, tmpKMeans = getThresholdClusteringKMeans(accuracyList, diversityMatrix[:, i], kmeansInit='strategic')\n",
    "            tmpThreshold = max(np.mean(diversityMatrix[:, i]), tmpThreshold)\n",
    "            thresholds.append(tmpThreshold)\n",
    "            kmeans.append(tmpKMeans)\n",
    "        fixedTeamDict['Threshold'] = thresholds\n",
    "        fixedTeamDict['KMeans'] = kmeans\n",
    "        \n",
    "        # calculate scaled diversity scores\n",
    "        scaledDiversityMeasures = list()\n",
    "        for i in range(len(diversityMetricsList)):\n",
    "            scaledDiversityMeasures.append(normalize01(diversityMatrix[:, i]))\n",
    "        scaledDiversityMatrix = np.stack(scaledDiversityMeasures, axis=1)\n",
    "        fixedTeamDict['ScaledDiversityMatrix'] = scaledDiversityMatrix\n",
    "        targetTeamSizeDict[str(oneTargetModel)][str(teamSize)] = fixedTeamDict\n",
    "        \n",
    "        for i, teamName in enumerate(fixedTeamDict['TeamList']):\n",
    "            for j in range(len(diversityMetricsList)):\n",
    "                targetTeamSizeFQDiversitySet = targetTeamSizeFQDict.get(diversityMetricsList[j], set())\n",
    "                targetTeamSizeFQOutDiversitySet = targetTeamSizeFQOutDict.get(diversityMetricsList[j], set())\n",
    "                if diversityMatrix[i, j] > round(thresholds[j], 3):\n",
    "                    targetTeamSizeFQDiversitySet.add(teamName)\n",
    "                else:\n",
    "                    targetTeamSizeFQOutDiversitySet.add(teamName)\n",
    "                targetTeamSizeFQDict[diversityMetricsList[j]] = targetTeamSizeFQDiversitySet\n",
    "                targetTeamSizeFQOutDict[diversityMetricsList[j]] = targetTeamSizeFQOutDiversitySet\n",
    "\n",
    "        targetFQDict[str(teamSize)] = targetTeamSizeFQDict\n",
    "        targetFQOutDict[str(teamSize)] = targetTeamSizeFQOutDict\n",
    "\n",
    "        \n",
    "    teamSelectedFQDict[str(oneTargetModel)] = targetFQDict\n",
    "    teamSelectedFQOutDict[str(oneTargetModel)] = targetFQOutDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3083f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "teamSelectedFQAllDict = dict()\n",
    "for j, dm in enumerate(diversityMetricsList):\n",
    "    teamSelectedFQAllDiversitySet = teamSelectedFQAllDict.get(dm, set())\n",
    "    for teamSize in range(2, len(models)):\n",
    "        teamSizeSelectedTeamsSet = set()\n",
    "        tmpTeamDict = dict() # teamName & Metric\n",
    "        for oneTargetModel in range(len(models)):\n",
    "            for teamName in teamSelectedFQDict[str(oneTargetModel)][str(teamSize)][dm]:\n",
    "                if teamName in tmpTeamDict:\n",
    "                    continue\n",
    "                tmpMetricList = list()\n",
    "                teamModelIdx = map(int, [modelName for modelName in teamName])\n",
    "                teamModelAcc = [tmpAccList[modelIdx].item() for modelIdx in teamModelIdx]\n",
    "                teamModelWeights = np.argsort(teamModelAcc)\n",
    "                tmpModelWeights = list()\n",
    "                for (k, modelName) in enumerate(teamName):\n",
    "                    fixedTeamDict = targetTeamSizeDict[modelName][str(teamSize)]\n",
    "                    for i, tmpTeamName in enumerate(fixedTeamDict['TeamList']):\n",
    "                        if tmpTeamName == teamName:\n",
    "                            tmpMetricList.append(fixedTeamDict['ScaledDiversityMatrix'][i, j])\n",
    "                            tmpModelWeights.append(teamModelWeights[k])\n",
    "                tmpTeamDict[teamName] = np.average(tmpMetricList, weights=tmpModelWeights)\n",
    "        if len(tmpTeamDict) > 0:\n",
    "            accuracyList = np.array([teamAccuracyDict[teamName] for teamName in tmpTeamDict])\n",
    "            metricList = np.array([tmpTeamDict[teamName] for teamName in tmpTeamDict])\n",
    "            tmpThreshold, _ = getThresholdClusteringKMeansCenter(accuracyList, metricList, kmeansInit='strategic')\n",
    "            for teamName in tmpTeamDict:\n",
    "                if tmpTeamDict[teamName] > tmpThreshold:\n",
    "                    teamSizeSelectedTeamsSet.add(teamName)\n",
    "        teamSelectedFQAllDiversitySet.update(teamSizeSelectedTeamsSet)\n",
    "    teamSelectedFQAllDict[dm] = teamSelectedFQAllDiversitySet\n",
    "\n",
    "\n",
    "# print the ensemble selection results\n",
    "for dm in diversityMetricsList:\n",
    "    print(dm, getNTeamStatistics(list(teamSelectedFQAllDict[dm]), teamAccuracyDict, minAcc, avgAcc, maxAcc, tmpAccList))\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
